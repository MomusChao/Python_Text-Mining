{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editor: Ching Tou, Chao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read stopword\n",
    "with open(\"stopword_chinese.txt\", 'r', encoding='UTF-8') as readstoplist:\n",
    "    stoptext = readstoplist.readlines()\n",
    "stoplist=[line.replace(\"\\n\",\"\") for line in stoptext]\n",
    "\n",
    "#split maintext\n",
    "f1 = open('TMBD_news_files_2000.txt', 'r', encoding='UTF-8')\n",
    "content_read = f1.read()\n",
    "content_split = content_read.split()\n",
    "\n",
    "\n",
    "#stopword removal\n",
    "for a in range(0,168):\n",
    "   for b in range(0,655851):\n",
    "    if content_split[b] == stoplist[a]:\n",
    "        content_split[b] = \"\"\n",
    "content_removal_1 = content_split\n",
    "\n",
    "#read punctuation\n",
    "with open(\"punctuation.txt\", 'r', encoding='UTF-8') as PunText:\n",
    "    punctext = PunText.readlines()\n",
    "punlist=[line.replace(\"\\n\",\"\") for line in punctext]\n",
    "\n",
    "for a in range(0,43):\n",
    "   for b in range(0,655851):\n",
    "    if content_removal_1[b] == punlist[a]:\n",
    "        content_removal_1[b] = \"\"\n",
    "content_clear_punctuation = content_removal_1\n",
    "\n",
    "# Rank term by frequency\n",
    "import collections\n",
    "counter=collections.Counter(content_clear_punctuation)\n",
    "dictionary_raw = dict(zip(counter.keys(), counter.values()))\n",
    "\n",
    "from collections import Counter\n",
    "c=Counter(dictionary_raw)\n",
    "dictionary_sort = c.most_common()\n",
    "\n",
    "del dictionary_sort[0]\n",
    "\n",
    "# Count Threshold\n",
    "CT1=0\n",
    "for a in range(0,37295):\n",
    "    CT1 = CT1 + dictionary_sort[a][1]\n",
    "import numpy as np\n",
    "CT2 = np.log10(CT1)\n",
    "\n",
    "# Avoid Noise from rarely occuring words\n",
    "dictionary_after_Threshold=list().\n",
    "\n",
    "for a in range(0,37295):\n",
    "    if CT2 <= dictionary_sort[a][1]:\n",
    "        dictionary_after_Threshold.append(dictionary_sort[a])\n",
    "\n",
    "## Document Frequency, DF\n",
    "# read document & save separately\n",
    "w, h = 2, 2000; #col & row\n",
    "Fragment = [[0 for x in range(w)] for y in range(h)] \n",
    "c = 0\n",
    "\n",
    "f1 = open('TMBD_news_files_2000.txt', 'r', encoding='UTF-8')\n",
    "for line in f1:\n",
    "    Fragment[c] = line\n",
    "    c = c+1\n",
    "\n",
    "#splite documents\n",
    "for i in range (0,2000):\n",
    "    Fragment[i] = Fragment[i].split( )\n",
    "\n",
    "# DF\n",
    "DF_list = list()\n",
    "count = 0\n",
    "\n",
    "for i in range (0,9119):\n",
    "    for j in range (0,2000):\n",
    "        if dictionary_after_Threshold[i][0] in Fragment[j]:\n",
    "            count = count + 1\n",
    "    DF_list.append(count)\n",
    "    count=0    \n",
    "\n",
    "# Inverse Document Frequemcy, IDF\n",
    "import numpy as np\n",
    "IDF_list = list()\n",
    "for dd in DF_list:\n",
    "    temp = np.log10(2000/dd)\n",
    "    IDF_list.append(temp)\n",
    "\n",
    "# Term Frequence, TF\n",
    "bone=dictionary_after_Threshold\n",
    "docdoc=[]\n",
    "for subdoc in Fragment:\n",
    "    docr = []\n",
    "    for ii in bone:\n",
    "        count = subdoc.count(ii[0])\n",
    "        docr.append(count)\n",
    "    docdoc.append(docr)\n",
    "\n",
    "# TF-IDF weighting\n",
    "TF_IDF=[]\n",
    "for doct in docdoc:\n",
    "    ttt=[]\n",
    "    for ind,sub in enumerate(doct):\n",
    "        ttt.append(sub*IDF_list[ind])\n",
    "    TF_IDF.append(ttt)\n",
    "\n",
    "# Vector Space Model - Cosine similarity\n",
    "from scipy import spatial\n",
    "Similarity=[]\n",
    "for ind,sub in enumerate(TF_IDF):\n",
    "    Similarity.append(1 - spatial.distance.cosine(TF_IDF[55],TF_IDF[ind]))\n",
    "\n",
    "Similarity_raw =[[idd,en] for idd,en in enumerate(Similarity)]\n",
    "\n",
    "# sort the result\n",
    "Similarity_sort =sorted(Similarity_raw, key = lambda x : x[1], reverse=True) \n",
    "\n",
    "# show the top 10\n",
    "Similarity_sort[1:11]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
